{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "dbcfab2b-a708-454b-bef6-20c8af96384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.stats import loguniform  # Para distribuciones logar√≠tmicas\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.stats import loguniform  # Para distribuciones logar√≠tmicas\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.stats import loguniform, uniform\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211d981d-d79e-4566-bc00-1d6c2780ad5c",
   "metadata": {},
   "source": [
    "### Optimizacion de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0dda5f-20ff-403f-ad14-8f4377b89205",
   "metadata": {},
   "source": [
    "| Modelo            | Ajuste de Hiperpar√°metros | Raz√≥n                                                                 |\n",
    "|-------------------|---------------------------|-----------------------------------------------------------------------|\n",
    "| **Na√Øve Bayes**   | **Grid Search**           | Pocos hiperpar√°metros (ej. `alpha` en `MultinomialNB`). Grid Search es r√°pido y suficiente para espacios peque√±os. |\n",
    "| **Regresi√≥n Log√≠stica** | **Grid Search** o **Random Search** | Grid Search si es simple (ej. probar `C` y `penalty`). Random Search si se a√±aden m√°s par√°metros (ej. `solver`, `tol`). |\n",
    "| **SVM**           | **Random Search**         | Modelo complejo con espacio de b√∫squeda grande (ej. `C`, `kernel`, `gamma`). Random Search es m√°s eficiente computacionalmente. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85335ac3-3ad6-4ad6-a80b-c431528264a8",
   "metadata": {},
   "source": [
    "## Naive Bayes optimizado con GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "23fb02f0-c25a-4791-be57-4176b34461b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor par√°metro encontrado: {'alpha': 0.1, 'fit_prior': False}\n",
      "Na√Øve Bayes Optimizado - Precisi√≥n en Test:\n",
      "Accuracy: 0.6101083032490975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.47      0.53       135\n",
      "           2       0.27      0.14      0.18        93\n",
      "           3       0.27      0.27      0.27       165\n",
      "           4       0.35      0.42      0.38       311\n",
      "           5       0.79      0.80      0.79       958\n",
      "\n",
      "    accuracy                           0.61      1662\n",
      "   macro avg       0.46      0.42      0.43      1662\n",
      "weighted avg       0.61      0.61      0.61      1662\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score, classification_report\n",
    "\n",
    "# Definir el modelo base\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Definir los hiperpar√°metros a probar\n",
    "param_grid = {\n",
    "    'alpha': [0.01, 0.1, 0.5, 1.0, 2.0, 5.0],  # Probar m√°s valores de alpha\n",
    "    'fit_prior': [True, False]  # Probar con y sin ajuste de prior\n",
    "}\n",
    "\n",
    "# Usar la m√©trica F1-macro para mejorar rendimiento en clases desbalanceadas\n",
    "scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "grid_search = GridSearchCV(nb_model, param_grid, cv=5, scoring=scorer, n_jobs=-1)\n",
    "\n",
    "# Ajustar el modelo con los datos de entrenamiento\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Obtener el mejor modelo y hacer predicciones\n",
    "best_nb_model = grid_search.best_estimator_\n",
    "y_pred_nb = best_nb_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluar el modelo optimizado\n",
    "print(\"Mejor par√°metro encontrado:\", grid_search.best_params_)\n",
    "print(\"Na√Øve Bayes Optimizado - Precisi√≥n en Test:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(classification_report(y_test, y_pred_nb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "99159cd7-c7a5-489c-976e-a44eb31588c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo Na√Øve Bayes optimizado guardado exitosamente en:\n",
      "C:\\Users\\Vicky\\Documents\\ML_Amazon_Reviews_Analysis\\models\\optimized_naive_bayes_20250408.pkl\n",
      "\n",
      "üîπ Mejores par√°metros: {'alpha': 0.1, 'fit_prior': False}\n",
      "üîπ Mejor score (CV): 0.4301\n",
      "üîπ Accuracy (test): 0.6101\n"
     ]
    }
   ],
   "source": [
    "# 2. Preparar todos los componentes para guardar\n",
    "model_components = {\n",
    "    'model': best_nb_model,  # El modelo optimizado\n",
    "    'grid_search': grid_search,  # Objeto completo de GridSearchCV\n",
    "    'vectorizer': vectorizer,  # El vectorizador TF-IDF\n",
    "    'label_encoder': le,  # El LabelEncoder\n",
    "    'metadata': {\n",
    "        'model_type': 'Optimized Na√Øve Bayes (GridSearchCV)',\n",
    "        'best_parameters': grid_search.best_params_,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'test_accuracy': accuracy_score(y_test, y_pred_nb),\n",
    "        'classification_report': classification_report(y_test, y_pred_nb, output_dict=True),\n",
    "        'training_date': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        'features': f\"{X_train_tfidf.shape[1]} features TF-IDF\",\n",
    "        'classes': list(le.classes_),\n",
    "        'cv_results': grid_search.cv_results_  # Todos los resultados de CV\n",
    "    }\n",
    "}\n",
    "\n",
    "# 3. Guardar todo en un archivo .pkl\n",
    "filename = f\"optimized_naive_bayes_{datetime.datetime.now().strftime('%Y%m%d')}.pkl\"\n",
    "full_path = os.path.join(model_path, filename)\n",
    "\n",
    "with open(full_path, 'wb') as file:\n",
    "    pickle.dump(model_components, file)\n",
    "\n",
    "print(f\"‚úÖ Modelo Na√Øve Bayes optimizado guardado exitosamente en:\\n{full_path}\")\n",
    "print(f\"\\nüîπ Mejores par√°metros: {grid_search.best_params_}\")\n",
    "print(f\"üîπ Mejor score (CV): {grid_search.best_score_:.4f}\")\n",
    "print(f\"üîπ Accuracy (test): {accuracy_score(y_test, y_pred_nb):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9ac041-44df-4da0-83a6-efb870ea964a",
   "metadata": {},
   "source": [
    "### Regresion Logistica optimizada con Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "ec86a465-e3c0-49e5-8f99-198f23a9a652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Mejor par√°metro encontrado (Grid Search): {'C': 1, 'penalty': 'l1', 'solver': 'saga'}\n",
      "üîπ Regresi√≥n Log√≠stica Optimizada (Grid Search) - Precisi√≥n en Test:\n",
      "Accuracy: 0.6660649819494585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.69      0.52      0.59       135\n",
      "           2       0.38      0.10      0.15        93\n",
      "           3       0.42      0.22      0.29       165\n",
      "           4       0.48      0.25      0.33       311\n",
      "           5       0.71      0.95      0.81       958\n",
      "\n",
      "    accuracy                           0.67      1662\n",
      "   macro avg       0.53      0.41      0.44      1662\n",
      "weighted avg       0.62      0.67      0.62      1662\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Definir el modelo base\n",
    "lr_model = LogisticRegression(max_iter=500)\n",
    "\n",
    "# Definir los hiperpar√°metros a probar\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # Control de regularizaci√≥n\n",
    "    'penalty': ['l1', 'l2'],  # Tipos de regularizaci√≥n\n",
    "    'solver': ['liblinear', 'saga']  # Solvers compatibles con l1 y l2\n",
    "}\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "grid_search = GridSearchCV(lr_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Ajustar el modelo con los datos de entrenamiento\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Obtener el mejor modelo y hacer predicciones\n",
    "best_lr_model_grid = grid_search.best_estimator_\n",
    "y_pred_lr_grid = best_lr_model_grid.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluar el modelo optimizado\n",
    "print(\"üîπ Mejor par√°metro encontrado (Grid Search):\", grid_search.best_params_)\n",
    "print(\"üîπ Regresi√≥n Log√≠stica Optimizada (Grid Search) - Precisi√≥n en Test:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr_grid))\n",
    "print(classification_report(y_test, y_pred_lr_grid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d18848-5f1b-4158-a7ff-e345a62dde27",
   "metadata": {},
   "source": [
    "### Regresion logistica optimizada con Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b0391c-9ac6-4cf8-8ee4-7c41f4cfa3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Definir el modelo base\n",
    "lr_model = LogisticRegression(max_iter=500)\n",
    "\n",
    "# Definir los hiperpar√°metros con rangos m√°s amplios\n",
    "param_dist = {\n",
    "    'C': np.logspace(-3, 3, 10),  # Valores en escala logar√≠tmica\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# Configurar RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(lr_model, param_distributions=param_dist, \n",
    "                                   n_iter=20, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "\n",
    "# Ajustar el modelo con los datos de entrenamiento\n",
    "random_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Obtener el mejor modelo y hacer predicciones\n",
    "best_lr_model_random = random_search.best_estimator_\n",
    "y_pred_lr_random = best_lr_model_random.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluar el modelo optimizado\n",
    "print(\"üîπ Mejor par√°metro encontrado (Random Search):\", random_search.best_params_)\n",
    "print(\"üîπ Regresi√≥n Log√≠stica Optimizada (Random Search) - Precisi√≥n en Test:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr_random))\n",
    "print(classification_report(y_test, y_pred_lr_random))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcb77ef-f261-4729-b079-de34e7464733",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_components = {\n",
    "    'model': best_lr_model_grid,  # El modelo optimizado\n",
    "    'grid_search': grid_search,  # Objeto completo de GridSearchCV\n",
    "    'vectorizer': vectorizer,  # El vectorizador TF-IDF\n",
    "    'label_encoder': le,  # El LabelEncoder\n",
    "    'metadata': {\n",
    "        'model_type': 'Optimized Logistic Regression (GridSearchCV)',\n",
    "        'best_parameters': grid_search.best_params_,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'test_accuracy': accuracy_score(y_test, y_pred_lr_grid),\n",
    "        'classification_report': classification_report(y_test, y_pred_lr_grid, output_dict=True),\n",
    "        'training_date': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        'features': f\"{X_train_tfidf.shape[1]} features TF-IDF\",\n",
    "        'classes': list(le.classes_),\n",
    "        'cv_results': grid_search.cv_results_  # Todos los resultados de CV\n",
    "    }\n",
    "}\n",
    "\n",
    "# 3. Guardar todo en un archivo .pkl\n",
    "filename = f\"optimized_logistic_regression_grid_{datetime.datetime.now().strftime('%Y%m%d')}.pkl\"\n",
    "full_path = os.path.join(model_path, filename)\n",
    "\n",
    "with open(full_path, 'wb') as file:\n",
    "    pickle.dump(model_components, file)\n",
    "\n",
    "print(f\"‚úÖ Modelo de Regresi√≥n Log√≠stica optimizado (GridSearch) guardado exitosamente en:\\n{full_path}\")\n",
    "print(f\"\\nüîπ Mejores par√°metros: {grid_search.best_params_}\")\n",
    "print(f\"üîπ Mejor score (CV): {grid_search.best_score_:.4f}\")\n",
    "print(f\"üîπ Accuracy (test): {accuracy_score(y_test, y_pred_lr_grid):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b3a92-edfb-4048-8b03-b04900bd048f",
   "metadata": {},
   "source": [
    "### SVM optimizado con Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dde77f-d236-41a1-8baa-fce748c2530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Definir el modelo base\n",
    "svm_model = SVC()\n",
    "\n",
    "# Definir los hiperpar√°metros con rangos amplios\n",
    "param_dist = {\n",
    "    'C': np.logspace(-3, 3, 10),  # Valores en escala logar√≠tmica (0.001 a 1000)\n",
    "    'kernel': ['linear', 'rbf', 'poly'],  # Probamos diferentes kernels\n",
    "    'gamma': ['scale', 'auto']  # Opciones est√°ndar para gamma\n",
    "}\n",
    "\n",
    "# Configurar RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(svm_model, param_distributions=param_dist, \n",
    "                                   n_iter=20, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "\n",
    "# Ajustar el modelo con los datos de entrenamiento\n",
    "random_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Obtener el mejor modelo y hacer predicciones\n",
    "best_svm_model = random_search.best_estimator_\n",
    "y_pred_svm = best_svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluar el modelo optimizado\n",
    "print(\"üîπ Mejor par√°metro encontrado (Random Search):\", random_search.best_params_)\n",
    "print(\"üîπ SVM Optimizado - Precisi√≥n en Test:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(classification_report(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431d3931-a9eb-464b-be68-8c5a47068bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preparar todos los componentes para guardar\n",
    "model_components = {\n",
    "    'model': best_lr_model_random,  # El modelo optimizado\n",
    "    'random_search': random_search,  # Objeto completo de RandomizedSearchCV\n",
    "    'vectorizer': vectorizer,  # El vectorizador TF-IDF\n",
    "    'label_encoder': le,  # El LabelEncoder\n",
    "    'metadata': {\n",
    "        'model_type': 'Optimized Logistic Regression (RandomizedSearchCV)',\n",
    "        'best_parameters': random_search.best_params_,\n",
    "        'best_score': random_search.best_score_,\n",
    "        'test_accuracy': accuracy_score(y_test, y_pred_lr_random),\n",
    "        'classification_report': classification_report(y_test, y_pred_lr_random, output_dict=True),\n",
    "        'training_date': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        'features': f\"{X_train_tfidf.shape[1]} features TF-IDF\",\n",
    "        'classes': list(le.classes_),\n",
    "        'param_distributions': param_dist,  # Distribuci√≥n de par√°metros usada\n",
    "        'random_state': 42  # Semilla aleatoria para reproducibilidad\n",
    "    }\n",
    "}\n",
    "\n",
    "# 3. Guardar todo en un archivo .pkl\n",
    "filename = f\"optimized_logistic_regression_random_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}.pkl\"\n",
    "full_path = os.path.join(model_path, filename)\n",
    "\n",
    "with open(full_path, 'wb') as file:\n",
    "    pickle.dump(model_components, file)\n",
    "\n",
    "print(f\"‚úÖ Modelo de Regresi√≥n Log√≠stica optimizado (Random Search) guardado exitosamente en:\\n{full_path}\")\n",
    "print(f\"\\nüîπ Mejores par√°metros: {random_search.best_params_}\")\n",
    "print(f\"üîπ Mejor score (CV): {random_search.best_score_:.4f}\")\n",
    "print(f\"üîπ Accuracy (test): {accuracy_score(y_test, y_pred_lr_random):.4f}\")\n",
    "print(f\"üîπ N√∫mero de iteraciones: {random_search.n_iter}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8d9d99-1d72-4f9c-b5ee-a7db377e608a",
   "metadata": {},
   "source": [
    "## Modelos mas avanzados\n",
    "### Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e74085d-b537-4f40-86d1-8b8176e696bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# 1. Cargar y preparar datos\n",
    "# Asumiendo que X e y est√°n definidos\n",
    "X['sentimiento'] = X['sentimiento'].map({'positivo': 1, 'negativo': -1, 'neutral': 0})\n",
    "X['sentimiento'] = pd.to_numeric(X['sentimiento'], errors='coerce').fillna(0)\n",
    "\n",
    "# 2. Dividir en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# 3. Funci√≥n para convertir tokens a formato compatible\n",
    "def tokens_to_dict(tokens_series):\n",
    "    return [dict(zip(tokens, [1]*len(tokens))) for tokens in tokens_series]\n",
    "\n",
    "# 4. Transformador personalizado para manejar datos dispersos\n",
    "class DenseTransformer:\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.toarray() if hasattr(X, 'toarray') else X\n",
    "\n",
    "# 5. Preprocesamiento adaptado\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(\n",
    "                max_features=5000,\n",
    "                stop_words='english',\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=5,\n",
    "                max_df=0.7)),\n",
    "            ('densify', DenseTransformer())\n",
    "        ]), 'text_limpio'),\n",
    "        ('num', StandardScaler(), ['helpful_vote', 'length', 'sentimiento']),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['verified_purchase']),\n",
    "        ('tokens', Pipeline([\n",
    "            ('to_dict', FunctionTransformer(tokens_to_dict)),\n",
    "            ('vectorize', DictVectorizer()),\n",
    "            ('densify', DenseTransformer())\n",
    "        ]), 'tokens')\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# 6. Pipeline final\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', HistGradientBoostingClassifier(\n",
    "        random_state=42,\n",
    "        early_stopping=True,\n",
    "        scoring='accuracy',\n",
    "        categorical_features=None  # Lo determinaremos despu√©s\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 7. Determinar √≠ndices de caracter√≠sticas categ√≥ricas\n",
    "preprocessor.fit(X_train)\n",
    "n_text_features = preprocessor.named_transformers_['text'].named_steps['tfidf'].get_feature_names_out().shape[0]\n",
    "n_num_features = 3  # helpful_vote, length, sentimiento\n",
    "n_cat_features = len(preprocessor.named_transformers_['cat'].get_feature_names_out())\n",
    "\n",
    "# Las caracter√≠sticas categ√≥ricas son las one-hot encoded (verified_purchase)\n",
    "categorical_indices = list(range(n_text_features + n_num_features, \n",
    "                               n_text_features + n_num_features + n_cat_features))\n",
    "\n",
    "# 8. Actualizar el pipeline con los √≠ndices categ√≥ricos\n",
    "pipeline.set_params(classifier__categorical_features=categorical_indices)\n",
    "\n",
    "# 9. Par√°metros para GridSearchCV (optimizados)\n",
    "param_grid = {\n",
    "    'classifier__learning_rate': [0.05, 0.1],\n",
    "    'classifier__max_iter': [100, 150],\n",
    "    'classifier__max_depth': [3, 5],\n",
    "    'classifier__min_samples_leaf': [5, 10],\n",
    "    'preprocessor__text__tfidf__max_features': [3000, 4000]\n",
    "}\n",
    "\n",
    "# 10. B√∫squeda de hiperpar√°metros\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 11. Entrenamiento\n",
    "print(\"Iniciando b√∫squeda de hiperpar√°metros...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 12. Evaluaci√≥n\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"\\nMejores par√°metros encontrados:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nMatriz de Confusi√≥n:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc89b7a-88b7-45f4-94b0-0db24a75b763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preparar los componentes para guardar\n",
    "model_components = {\n",
    "    'pipeline': pipeline,  # Tu pipeline completo\n",
    "    'metadata': {\n",
    "        'model_type': 'Random Forest Pipeline',\n",
    "        'features_used': list(X_train.columns),\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'training_date': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        'classes': list(y.unique())\n",
    "    }\n",
    "}\n",
    "\n",
    "# 3. Guardar el modelo\n",
    "filename = f\"random_forest_pipeline_{datetime.datetime.now().strftime('%Y%m%d')}.pkl\"\n",
    "full_path = os.path.join(model_path, filename)\n",
    "\n",
    "with open(full_path, 'wb') as file:\n",
    "    pickle.dump(model_components, file)\n",
    "\n",
    "print(f\"Modelo guardado exitosamente en: {full_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc38ac2-8684-47bd-95f9-1495efa2c6e9",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2899eb9-f923-4d13-8317-065eefae01c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "classes = df_reviews_clean['rating'].unique()\n",
    "weights = class_weight.compute_sample_weight('balanced', df_reviews_clean['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b98de-756e-43ef-b698-dd84443def94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Paso 1: Re-mapear las etiquetas para que empiecen en 0\n",
    "y_train_remap = np.array(y_train) - 1  # [1, 2, 3, 4, 5] ‚Üí [0, 1, 2, 3, 4]\n",
    "y_test_remap = np.array(y_test) - 1    # Aplicar lo mismo a los datos de test\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'multi:softmax',\n",
    "        'num_class': 5,  # N√∫mero de clases (ahora son [0, 1, 2, 3, 4])\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "    }\n",
    "    \n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(X_train_tfidf, y_train_remap)  # Usar y_train_remap\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    return f1_score(y_test_remap, y_pred, average='macro')  # Evaluar con y_test_remap\n",
    "\n",
    "# Paso 2: Ejecutar la optimizaci√≥n\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Paso 3: Mostrar mejores par√°metros y resultados\n",
    "print(\"Mejores par√°metros:\", study.best_params)\n",
    "\n",
    "# (Opcional) Entrenar el modelo final con los mejores par√°metros\n",
    "best_model = XGBClassifier(**study.best_params, objective='multi:softmax', num_class=5)\n",
    "best_model.fit(X_train_tfidf, y_train_remap)\n",
    "y_pred = best_model.predict(X_test_tfidf)\n",
    "\n",
    "# Generar reporte de clasificaci√≥n\n",
    "y_pred_original = y_pred + 1  \n",
    "print(classification_report(y_test, y_pred_original))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6080c4-6dfa-4542-96d9-0d74dc60ca79",
   "metadata": {},
   "source": [
    "### An√°lisis Comparativo de M√©tricas\n",
    "\n",
    "#### **1. Rendimiento General por Modelo**\n",
    "| Modelo                     | Accuracy | F1-Score (Macro) | F1-Score (Weighted) | Tiempo Estimado |\n",
    "|----------------------------|----------|-------------------|----------------------|-----------------|\n",
    "| Na√Øve Bayes Optimizado      | 0.61     | 0.43              | 0.61                 | ‚ö° M√°s r√°pido   |\n",
    "| Regresi√≥n Log√≠stica (Grid)  | **0.67** | 0.44              | **0.62**             | ‚è±Ô∏è Medio       |\n",
    "| SVM Optimizado              | 0.65     | 0.42              | 0.60                 | üêå M√°s lento   |\n",
    "| **Gradient Boosting**       | 0.65     | 0.37              | 0.58                 | ‚è±Ô∏è Medio       |\n",
    "| **XGBoost Optimizado**      | 0.56     | 0.19              | 0.46                 | ‚è±Ô∏è Medio       |\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Problemas Comunes Identificados**\n",
    "- **Sesgo hacia la clase mayoritaria (5)**:  \n",
    "  Todos los modelos (especialmente XGBoost) tienen recall >90% para la clase 5, pero <20% para las clases 2, 3 y 4.  \n",
    "  - *Ejemplo*: Gradient Boosting tiene **0% recall en clase 2** (no detecta ning√∫n caso).\n",
    "\n",
    "- **Overfitting en XGBoost**:  \n",
    "  A pesar de ajustar hiperpar√°metros, su F1-score macro es muy bajo (0.19), lo que sugiere que est√° memorizando ruido en los datos.\n",
    "\n",
    "- **Precisi√≥n vs Recall desbalanceados**:  \n",
    "  - Gradient Boosting tiene alta precisi√≥n en clase 1 (82%) pero bajo recall (34%), indicando falsos negativos.  \n",
    "  - SVM tiene mejor equilibrio (ej: clase 1 con F1=0.55).\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Recomendaciones por Modelo**\n",
    "##### **Gradient Boosting**\n",
    "- **Ventajas**:  \n",
    "  - Buen accuracy (65%) similar a SVM, pero m√°s r√°pido.  \n",
    "  - Detecta algo de se√±al en clases minoritarias (ej: F1=0.31 en clase 3).  \n",
    "- **Problemas**:  \n",
    "  - Colapso en clase 2 (F1=0.00).  \n",
    "  - Matriz de confusi√≥n muestra confusi√≥n entre clases 1-5 y 3-5.  \n",
    "- **Acciones**:  \n",
    "  - Aumentar `min_samples_leaf` (evitar overfitting en hojas).  \n",
    "  - Usar `class_weight='balanced'` o oversampling (SMOTE).  \n",
    "\n",
    "##### **XGBoost**\n",
    "- **Ventajas**:  \n",
    "  - Arquitectura flexible (permite ajustes finos).  \n",
    "- **Problemas**:  \n",
    "  - P√©simo rendimiento en clases 1-4 (F1 <0.10).  \n",
    "  - Posiblemente necesita m√°s datos o features mejor dise√±adas.  \n",
    "- **Acciones**:  \n",
    "  - Reducir complejidad (`max_depth=5`, `gamma=0.5`).  \n",
    "  - Usar `scale_pos_weight` para balancear clases.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Comparaci√≥n con Modelos Previos**\n",
    "| M√©trica               | Mejor Modelo (Regresi√≥n Log√≠stica) | Gradient Boosting | XGBoost |\n",
    "|-----------------------|------------------------------------|-------------------|---------|\n",
    "| **Accuracy**          | **0.67**                           | 0.65              | 0.56    |\n",
    "| **F1-Score (Macro)**  | **0.44**                           | 0.37              | 0.19    |\n",
    "| **Recall Clase 2**    | 0.10                               | **0.00**          | 0.01    |\n",
    "| **Velocidad**         | ‚è±Ô∏è Medio                          | ‚è±Ô∏è Medio         | ‚è±Ô∏è Medio |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346301ae-364d-4c77-b734-d7a5f7a2e2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebed804-da71-4cb9-8e75-481a136fae05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
