{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e8859d0-3e9c-4288-be9d-e28e97e2fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, make_scorer, f1_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import loguniform\n",
    "import random\n",
    "\n",
    "# Configuracion\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "635419e9-cc4a-449c-906c-8b7a1c9e38fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestra guardada en C:\\Users\\Vicky\\Documents\\ML_Amazon_Reviews\\data_sample\\raw/dataset_sample_final.jsonl, tama침o: 7015 filas\n",
      "DataFrame creado con 7015 filas.\n"
     ]
    }
   ],
   "source": [
    "# Par치metros\n",
    "input_file = r\"C:\\Users\\Vicky\\Desktop\\All_Beauty.jsonl\"\n",
    "output_file = r\"C:\\Users\\Vicky\\Documents\\ML_Amazon_Reviews\\data_sample\\raw/dataset_sample_final.jsonl\"\n",
    "sample_fraction = 0.01\n",
    "random_seed = 42\n",
    "\n",
    "# Contar l칤neas totales y extraer muestra\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    total_lines = sum(1 for _ in f)\n",
    "sample_size = max(1, int(total_lines * sample_fraction))\n",
    "selected_lines = sorted(random.sample(range(total_lines), sample_size))\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f_in, open(output_file, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for i, line in enumerate(f_in):\n",
    "        if i in selected_lines:\n",
    "            f_out.write(line)\n",
    "\n",
    "print(f\"Muestra guardada en {output_file}, tama침o: {sample_size} filas\")\n",
    "\n",
    "# Cargar el JSONL en un DataFrame\n",
    "df_sample = pd.read_json(output_file, lines=True)\n",
    "print(f\"DataFrame creado con {len(df_sample)} filas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e5e188b-7a45-40c7-b827-9a347015c8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vicky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vicky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Limpieza y preprocesamiento\n",
    "df_reviews_clean = df_sample.copy().dropna().drop(columns=['images', 'asin', 'parent_asin', 'user_id', 'timestamp']).drop_duplicates()\n",
    "df_reviews_clean['text'] = df_reviews_clean['text'].apply(str) + ' ' + df_reviews_clean['title'].apply(str)\n",
    "df_reviews_clean['text'] = df_reviews_clean['text'].str.lower()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    if not isinstance(texto, str):\n",
    "        return \"\"\n",
    "    texto_limpio = re.sub(r'[^a-zA-Z치칠칤칩칰칲침츼칄칈칍칔칖칌\\s]', '', texto)\n",
    "    palabras = texto_limpio.split()\n",
    "    palabras_filtradas = [palabra for palabra in palabras if palabra not in stop_words]\n",
    "    return ' '.join(palabras_filtradas)\n",
    "\n",
    "df_reviews_clean['text_limpio'] = df_reviews_clean['text'].apply(limpiar_texto)\n",
    "df_reviews_clean = df_reviews_clean.drop(columns=['text'])\n",
    "\n",
    "# Clasificaci칩n del sentimiento\n",
    "df_reviews_clean['sentimiento'] = pd.cut(df_reviews_clean['rating'], bins=[0, 2, 3, 5], labels=['negativo', 'neutral', 'positivo'], right=True)\n",
    "\n",
    "nltk.download('punkt')\n",
    "df_reviews_clean['tokens'] = df_reviews_clean['text_limpio'].apply(lambda x: word_tokenize(str(x).lower()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fef1373-0048-4532-adb9-7328cfe77c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisi칩n de datos\n",
    "train_df, test_df = train_test_split(df_reviews_clean, test_size=0.2, random_state=42, stratify=df_reviews_clean['sentimiento'])\n",
    "\n",
    "# Vectorizaci칩n TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=10_000)\n",
    "X_train_tfidf = vectorizer.fit_transform(train_df['text_limpio']).astype('float32')\n",
    "X_test_tfidf = vectorizer.transform(test_df['text_limpio'])\n",
    "y_train = train_df['rating']\n",
    "y_test = test_df['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edf932d5-da8e-4464-8644-9e7a593d9830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de guardado de modelos\n",
    "model_path = r'C:\\Users\\Vicky\\Documents\\ML_Amazon_Reviews\\models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a97b1f64-f38d-433d-a9d2-22b218b772d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "游댳 Mejor par치metro encontrado (Grid Search): {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "游댳 Regresi칩n Log칤stica Optimizada (Grid Search) - Precisi칩n en Test:\n",
      "Accuracy: 0.7317073170731707\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.69      0.68       204\n",
      "           2       0.41      0.15      0.22        88\n",
      "           3       0.47      0.29      0.36       106\n",
      "           4       0.46      0.25      0.32       164\n",
      "           5       0.80      0.96      0.87       832\n",
      "\n",
      "    accuracy                           0.73      1394\n",
      "   macro avg       0.56      0.47      0.49      1394\n",
      "weighted avg       0.69      0.73      0.70      1394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Definir el modelo base\n",
    "lr_model = LogisticRegression(max_iter=500)\n",
    "\n",
    "# Definir los hiperpar치metros a probar\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # Control de regularizaci칩n\n",
    "    'penalty': ['l1', 'l2'],  # Tipos de regularizaci칩n\n",
    "    'solver': ['liblinear', 'saga']  # Solvers compatibles con l1 y l2\n",
    "}\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "grid_search = GridSearchCV(lr_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Ajustar el modelo con los datos de entrenamiento\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Obtener el mejor modelo y hacer predicciones\n",
    "best_lr_model_grid = grid_search.best_estimator_\n",
    "y_pred_lr_grid = best_lr_model_grid.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluar el modelo optimizado\n",
    "print(\"游댳 Mejor par치metro encontrado (Grid Search):\", grid_search.best_params_)\n",
    "print(\"游댳 Regresi칩n Log칤stica Optimizada (Grid Search) - Precisi칩n en Test:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr_grid))\n",
    "print(classification_report(y_test, y_pred_lr_grid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639b4963-7230-4f09-8dd2-607c78965a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8745d669-540f-4b43-8a1a-f73fcac9a0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfa8c0a-6d6f-4e48-8b94-716d2e9cf715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8dc85e-eab3-4e34-a2cc-1ad8a2705d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
